{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Translating Tweets"
      ],
      "metadata": {
        "id": "ZnV1eExVhA2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cardiffnlp/xlm-t"
      ],
      "metadata": {
        "id": "Gd-fnHF6BwBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def process_language(language):\n",
        "    base_dir = \"/content/xlm-t/data/sentiment\"\n",
        "    lang_path = os.path.join(base_dir, language)\n",
        "\n",
        "    all_rows = []\n",
        "\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        text_file_path = os.path.join(lang_path, f\"{split}_text.txt\")\n",
        "        label_file_path = os.path.join(lang_path, f\"{split}_labels.txt\")\n",
        "\n",
        "        if not os.path.exists(text_file_path) or not os.path.exists(label_file_path):\n",
        "            print(f\"Missing files for split '{split}' in {language}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        with open(text_file_path, encoding=\"utf-8\") as text_file, \\\n",
        "             open(label_file_path, encoding=\"utf-8\") as label_file:\n",
        "            texts = text_file.read().splitlines()\n",
        "            labels = label_file.read().splitlines()\n",
        "\n",
        "        for text, label in zip(texts, labels):\n",
        "            all_rows.append({\n",
        "                \"original_text\": text,\n",
        "                \"label\": int(label),\n",
        "                \"Split\": split\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    output_path = os.path.join(base_dir, f\"{language}.csv\")\n",
        "    df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Saved combined dataset to: {output_path}\")"
      ],
      "metadata": {
        "id": "NPTi8WhqDh9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_language(\"hindi\")\n",
        "process_language(\"arabic\")\n",
        "process_language(\"french\")\n",
        "process_language(\"german\")\n",
        "process_language(\"italian\")\n",
        "process_language(\"portuguese\")\n",
        "process_language(\"spanish\")"
      ],
      "metadata": {
        "id": "fnrbNL-RDl1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoAOmx7wBbSQ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# API setup\n",
        "os.environ[\"AI_SANDBOX_KEY\"] = \"6662b94a6403410284b9271b8f2afa4c\"\n",
        "sandbox_api_key = os.environ.get('AI_SANDBOX_KEY')\n",
        "\n",
        "sandbox_endpoint = \"https://api-ai-sandbox.princeton.edu/\"\n",
        "sandbox_api_version = \"2025-03-01-preview\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=sandbox_api_key,\n",
        "    azure_endpoint=sandbox_endpoint,\n",
        "    api_version=sandbox_api_version\n",
        ")\n",
        "\n",
        "prompt = '''\n",
        "Translate the text below to English. Keep the translation as close to the original in tone and style as you can.\n",
        "'''\n",
        "\n",
        "csv_dir = \"/content/xlm-t/data/sentiment\"\n",
        "output_dir = os.path.join(csv_dir, \"translated\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def translate(language):\n",
        "    input_path = os.path.join(csv_dir, f\"{language}.csv\")\n",
        "    output_path = os.path.join(output_dir, f\"{language}_gpt4o_translations.csv\")\n",
        "\n",
        "    with open(input_path, newline='', encoding=\"utf-8\") as infile, \\\n",
        "         open(output_path, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
        "\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = [\"row_index\", \"original_text\", \"translated_text\", \"label\", \"Split\"]\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for idx, row in enumerate(reader):\n",
        "            original_text = row[\"original_text\"]\n",
        "            label = row[\"label\"]\n",
        "            split = row.get(\"Split\", \"unspecified\")  # default to \"unspecified\" if missing\n",
        "            full_prompt = f\"{prompt}\\n\\n{original_text}\"\n",
        "\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    temperature=0,\n",
        "                    max_tokens=500,\n",
        "                    top_p=0.1,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                        {\"role\": \"user\", \"content\": full_prompt},\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                translated_text = response.choices[0].message.content.strip()\n",
        "\n",
        "                writer.writerow({\n",
        "                    \"row_index\": idx,\n",
        "                    \"original_text\": original_text,\n",
        "                    \"translated_text\": translated_text,\n",
        "                    \"label\": label,\n",
        "                    \"Split\": split\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error on row {idx}: {e}\")\n",
        "                writer.writerow({\n",
        "                    \"row_index\": idx,\n",
        "                    \"original_text\": original_text,\n",
        "                    \"translated_text\": f\"ERROR: {e}\",\n",
        "                    \"label\": label,\n",
        "                    \"Split\": split\n",
        "                })\n",
        "\n",
        "            time.sleep(0.3)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    languages = [\"portuguese\", \"hindi\"]\n",
        "    for lang in languages:\n",
        "        translate(lang)\n",
        "        print(f\"Done with {lang}\")\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "YsWj_R1zgj-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "vTMOi7RWhPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil"
      ],
      "metadata": {
        "id": "RCmMDPxvgdqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df = df[['translated_text', 'label', 'Split']].rename(columns={'translated_text': 'text'})\n",
        "    df = df.dropna()\n",
        "    df = df[~df['text'].str.startswith(\"ERROR:\")]  # skip translation errors\n",
        "    df = df[df['label'].isin([0, 1, 2])]\n",
        "    return df\n",
        "\n",
        "def tokenize_function(example, tokenizer):\n",
        "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = predictions.argmax(axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    report = classification_report(labels, preds, output_dict=True)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': report['weighted avg']['precision'],\n",
        "        'recall': report['weighted avg']['recall'],\n",
        "        'f1': report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    return plt"
      ],
      "metadata": {
        "id": "k1NIIJZxgeoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_sentiment_model(language):\n",
        "    print(f\"\\nProcessing language: {language}\")\n",
        "    base_dir = \"/content/xlm-t/data/sentiment/translated\"\n",
        "    df = pd.read_csv(f\"{base_dir}/{language}_gpt4o_translations.csv\")\n",
        "\n",
        "    model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "    data = preprocess_data(df)\n",
        "\n",
        "    train_df = data[data['Split'] == 'train']\n",
        "    val_df = data[data['Split'] == 'val']\n",
        "    test_df = data[data['Split'] == 'test']\n",
        "\n",
        "    train_ds = Dataset.from_pandas(train_df[['text', 'label']])\n",
        "    val_ds = Dataset.from_pandas(val_df[['text', 'label']])\n",
        "    test_ds = Dataset.from_pandas(test_df[['text', 'label']])\n",
        "\n",
        "    train_ds = train_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
        "    val_ds = val_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
        "    test_ds = test_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{language}_gpt4o_temp\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=15,\n",
        "        weight_decay=0.1,\n",
        "        logging_steps=10,\n",
        "        logging_dir=f\"./logs_{language}_gpt4o\",\n",
        "        report_to=\"none\",\n",
        "        metric_for_best_model=\"eval_accuracy\",\n",
        "        warmup_ratio=0.1,\n",
        "    )\n",
        "\n",
        "    early_stopping_callback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=2,\n",
        "        early_stopping_threshold=0.001\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[early_stopping_callback]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Evaluating on test set...\")\n",
        "    test_metrics = trainer.evaluate(test_ds)\n",
        "\n",
        "    predictions = trainer.predict(test_ds)\n",
        "    preds = predictions.predictions.argmax(-1)\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    correct_predictions = (preds == labels).sum()\n",
        "    total_samples = len(labels)\n",
        "\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    class_report = classification_report(labels, preds, target_names=class_names, output_dict=True)\n",
        "\n",
        "    test_metrics.update({\n",
        "        \"detailed_metrics\": class_report,\n",
        "        \"total_samples\": total_samples,\n",
        "        \"correct_predictions\": int(correct_predictions),\n",
        "        \"confusion_matrix\": cm.tolist()\n",
        "    })\n",
        "\n",
        "    # Save confusion matrix\n",
        "    plt = plot_confusion_matrix(cm, class_names)\n",
        "    cm_path = f\"{language}_gpt4o_confusion_matrix.png\"\n",
        "    plt.savefig(cm_path)\n",
        "    plt.close()\n",
        "    print(f\"Saved confusion matrix to {cm_path}\")\n",
        "\n",
        "    # Save JSON results\n",
        "    out_file = f\"{language}_gpt4o_results.json\"\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump(test_metrics, f, indent=2)\n",
        "    print(f\"Saved results to {out_file}\")\n",
        "\n",
        "    # Clean temp outputs\n",
        "    temp_path = f\"./results_{language}_gpt4o_temp\"\n",
        "    if os.path.exists(temp_path):\n",
        "        shutil.rmtree(temp_path)\n",
        "        print(f\"Cleaned up {temp_path}\")"
      ],
      "metadata": {
        "id": "d-cXlhKVg4UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"french\")"
      ],
      "metadata": {
        "id": "-v91T84IisUV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"german\")"
      ],
      "metadata": {
        "id": "uFjTSLTZ2kuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"spanish\")"
      ],
      "metadata": {
        "id": "H1syfHq22mva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"italian\")"
      ],
      "metadata": {
        "id": "gd8GJ9DY2o6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"arabic\")"
      ],
      "metadata": {
        "id": "uFiASxAf2o4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"hindi\")"
      ],
      "metadata": {
        "id": "_lA96kK1TSXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_sentiment_model(\"portuguese\")"
      ],
      "metadata": {
        "id": "WJZrszQ7TVEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}